{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PATTERN RECOGNITION\n",
    "\n",
    "## Assignment 5\n",
    "\n",
    "\n",
    "#### 1. Derive the restricted Boltzmann machine algorithm that you will implement, and explain your derivation. Implement the training and inference algorithms for RBM. Train RBMs with 20, 100 and 500 hidden nodes to generate MNIST images using the training data set. Generate MNIST images from the ones in the testing data set that have 20%, 50% and 80% pixels missing/removed. You are free to choose whether you want to use binary nodes or floating point nodes, but the derivation has to match the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Derivation\n",
    "\n",
    "The goal of boltzmann machine is to model a set of observed data in terms of a set of visible random variables $v$ and a set of latent random variables $h$. Due to the relationship between boltzmann machines nad neural netowrks, the random variables are known as units. The role of these visible units is to approximate the true distribution of the data and the role of latent variables, is that they extend the expressiveness of the model by capturing underlying features in the observed data. The latent variables are the hidden units, as we cannot get them directly from the observed data and are marginalized over to obtain the likelihood of the observed data\n",
    "\n",
    "$ p(v;\\theta) = \\sum_h{p(v,h;\\theta)}$\n",
    "\n",
    "where $p(v,h;\\theta) $ is the jojnt probability distribution over the visible and hidden units based on the current model parameters $\\theta$.\n",
    "The general boltzmann machine defines $p(v,h;\\theta) $ through a set of weighted, symmetric connextions between all visible and hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The restricted Boltzmann machine is an energy-based model with the joint probability distribution speciÔ¨Åed by its energy function:\n",
    "\\begin{equation}\n",
    "P(v,h) = \\frac{1}{Z} e^{-E(v,h)}\n",
    "\\end{equation}\n",
    "\n",
    "The Energy function for RBM is defined as:\n",
    "\\begin{equation}\n",
    "E(v,h) = -b^{T} v - c^{T} h - v^{T} W h \n",
    "\\end{equation}\n",
    "\n",
    "and Z is the partition function which is a normalizing constant given as:\n",
    "\\begin{equation}\n",
    "Z = \\Sigma_{v} \\Sigma_{h} e^{-E(v,h)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In the case of restricted Boltzmann machines, Long and Servedio (2010) formally proved that the partition function Z is intractable. The intractable partition function Z implies that the normalized joint probability distribution P (v) is also intractable to evaluate. Though P(v) is intractable, the bipartite graph structure of the RBM has the special property that visible and hidden units are conditionally independent given one-another. Therefore:\n",
    "\\begin{equation}\n",
    "P(h|v) = \\frac{P(h,v)}{P(v)}\n",
    "       = \\frac{1}{P(v)} \\frac{1}{Z} exp\\{b^{T} v + c^{T} h + v^{T} W h\\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    = \\frac{1}{Z'} exp\\{c^T h + v^{T} W h\\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    = \\frac{1}{Z'} exp\\{\\Sigma_j c_j h_j + \\Sigma_j v^T W_j h_j \\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    = \\frac{1}{Z'} \\Pi_j exp \\{ c_j h_j + v^T W_j h_j\\}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(h_j = 1,v) = \\frac{\\hat{P}(h_j = 1,v)}{\\hat{P}(h_j = 0,v) + \\hat{P}(h_j = 1,v)}\n",
    "             = \\frac{exp\\{c_j + v^T W_j \\}}{exp\\{0\\} + exp\\{c_j + v^T W_j \\}}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore:\n",
    "\\begin{equation}\n",
    "P(h_j = 1|v) = \\sigma(c_j + v^T W_j )\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Similary from Eq 8 we can say that:\n",
    "\\begin{equation}\n",
    "P(v|h) = \\frac{1}{Z'} \\Pi_k exp \\{b_k + h^T W_k \\}\n",
    "\\end{equation}\n",
    "Therefore:\n",
    "\\begin{equation}\n",
    "P(v_k = 1|h) = \\sigma(b_k + h^T W_k)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (<ipython-input-170-2a678245a9fb>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-170-2a678245a9fb>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train= loadlocal_mnist(images_path='train-images-idx3-ubyte',\n",
    "                                  labels_path='train-labels-idx1-ubyte')\n",
    "X_test, y_test=loadlocal_mnist(images_path='t10k-images-idx3-ubyte',\n",
    "                                labels_path='t10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "X_dim = MNIST.train.images.shape[1]\n",
    "y_dim = MNIST.train.labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-89ffff0ca221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnewX2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewY2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mMNIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# #Removing 50% of pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "# Removing 20% pixels\n",
    "import cv2\n",
    "\n",
    "newX,newY = MNIST.test.images.shape[1], MNIST.test.images.shape[0]*0.8\n",
    "newX1,newY1 = MNIST.test.images.shape[1], MNIST.test.images.shape[0]*0.5\n",
    "newX2,newY2 = MNIST.test.images.shape[1], MNIST.test.images.shape[0]*0.2\n",
    "\n",
    "# for i in MNIST20.test.images:\n",
    "    \n",
    "\n",
    "\n",
    "# #Removing 50% of pixels\n",
    "# MNIST50= cv2.resize(MNIST.test.images,(int(newX1),int(newY1)))\n",
    "\n",
    "# # #Removing 80% of pixels\n",
    "# MNIST80=cv2.resize(MNIST.test.images,(int(newX2),int(newY2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size=16\n",
    "h_dim= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(X_dim, h_dim) * 0.001\n",
    "a = np.random.randn(h_dim) * 0.001\n",
    "b = np.random.randn(X_dim) * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def infer(x):\n",
    "    return sigm(x @ W)\n",
    "\n",
    "def generate(H):\n",
    "    return sigm(H @ W.T)\n",
    "\n",
    "alpha=0.1\n",
    "K = 10\n",
    "\n",
    "for t in range(1, 1001):\n",
    "    X_mb = (MNIST.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "    g = 0\n",
    "    g_a = 0\n",
    "    g_b = 0\n",
    "\n",
    "    for v in X_mb:\n",
    "        \n",
    "        h = infer(v)\n",
    "\n",
    "        v_prime = np.copy(v)\n",
    "\n",
    "        for k in range(K):\n",
    "            h_prime = np.random.binomial(n=1, p=infer(v_prime))\n",
    "            v_prime = np.random.binomial(n=1, p=generate(h_prime))\n",
    "\n",
    "        \n",
    "        h_prime = infer(v_prime)\n",
    "\n",
    "        \n",
    "        grad_w = np.outer(v, h) - np.outer(v_prime, h_prime)\n",
    "        grad_a = h - h_prime\n",
    "        grad_b = v - v_prime\n",
    "\n",
    "       \n",
    "        g += grad_w\n",
    "        g_a += grad_a\n",
    "        g_b += grad_b\n",
    "\n",
    "   \n",
    "    g *= 1 / mb_size\n",
    "    g_a *= 1 / mb_size\n",
    "    g_b *= 1 / mb_size\n",
    "    W += alpha * g\n",
    "    a += alpha * g_a\n",
    "    b += alpha * g_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples, size, name):\n",
    "    size = int(size)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(size, size), cmap='Greys_r')\n",
    "\n",
    "    plt.savefig('out/{}.png'.format(name), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    X = (MNIST.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "\n",
    "    H = np.random.binomial(n=1, p=infer(X))\n",
    "#     plot(H, np.sqrt(h_dim), 'H')\n",
    "\n",
    "    X_recon = (generate(H) > 0.5).astype(np.float)\n",
    "    plot(X_recon, np.sqrt(X_dim), 'V')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(MNIST.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Hidden nodes\n",
    "\n",
    "<img src=\"out/20Nodes.png\"></img>\n",
    "\n",
    "## 100 Hidden nodes\n",
    "\n",
    "<img src=\"out/100Nodes.png\"></img>\n",
    "\n",
    "## 500 Hidden Nodes\n",
    "\n",
    "<img src=\"out/500Nodes.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VAE\n",
    "\n",
    "#### Derive the variational autoencoder algorithm that you will implement, and explain your derivation. Implement the training and inference algorithms for VAE.  Train VAE with 2, 8 and 16 code units to encode MNIST images using the training data set. The neural network will be 784 input -> 256 hidden -> 2/8/16 code -> 256 hidden -> 784 output. Then  use the 2 code -> 256 hidden -> 784 output part of the trained network with 2 code units to generate images by varying each code unit from -3 to 3. You are free to choose the other parameters.\n",
    "\n",
    "VAE (Variational autoencoder) is rooted in bayesian inference; it wants to model the underlying probability distribution of data so that it can sample new data points from that distribution. VAE uses latent variable, hence it is an expressive model.\n",
    "\n",
    "NOTATIONS:\n",
    "\n",
    "- $ X $ Data input for the model <br>\n",
    "- $ z $ Latent variable <br>\n",
    "- $ P(X) $ Probability distribution of the data <br>\n",
    "- $ P(z) $ Probability distribution of latent variable <br>\n",
    "- $ P(X|z) $ Probability distribution of data given latent variable\n",
    "\n",
    "Our main goal is to model the data, hence we need to find $P(X)$. THis can be done as follows:\n",
    "\n",
    "$ P(X) = \\int{P(X|z) P(z) dz}$\n",
    "\n",
    "that is we marginalize the joint probability from the latent variable.\n",
    "\n",
    "Now we need $P(X,z)$, the idea is to infer $ P(z) $ using $ P(z|X) $. For this we use a method called Variational Inference. The main idea of Variational Inference is to suggest the inference by thinking it as an optimization problem. This can be done by modeling the true distribution $P(z|X)$ using distribution such as Gaussian, and minimize the distribution using KL divergence, which tells us difference between P and Q.\n",
    "\n",
    "Now we have to infer $ P(z|X)$ using $Q(z|X)$\n",
    "\n",
    "We want to infer $P(z|X)$ using $Q(z|X)$, the KL divergence then formulated as follows:\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)‚à•P(z|X)] = \\Sigma_z Q(z|X) log \\frac{Q(z|X)}{P(z|X)}\n",
    "                      = E[log Q(z|X) - log P(z|X)]\n",
    "\\end{equation}\n",
    "\n",
    "By Baye's rule:\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)‚à•P(z|X)] = E[log Q(z|X) - log \\frac{P(X|z) P(z)}{P(X)}]\n",
    "= E[log Q(z|X) ‚àí log P(X|z) ‚àí log P(z) + log P(X)]\n",
    "\\end{equation}\n",
    "\n",
    "The expectation is over z and P(X) doesn‚Äôt depend on z, so we could move it outside of the expectation:\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)‚à•P(z|X)] = E[log Q(z|X) ‚àí log P(X|z) ‚àí log P(z) ] + log P(X)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "D_{KL}[Q(z|X)‚à•P(z|X)] - log P(X)  = E[log Q(z|X) ‚àí log P(X|z) ‚àí log P(z)]\n",
    "\\end{equation}\n",
    "\n",
    "The right hand side of Eq 15 can be expressed as another KL divergence (Rearranging signs):\n",
    "\\begin{equation}\n",
    "log P(X) - D_{KL}[Q(z|X)‚à•P(z|X)] = E[log P(X|z) - (log Q(z|X) ‚àí log P(z))]\n",
    "= E[log P(X|z)] - E[log Q(z|X) - log P(z)]\n",
    "\\end{equation}\n",
    "This is the VAE objective function:\n",
    "\\begin{equation}\n",
    "log P(X) - D_{KL}[Q(z|X)‚à•P(z|X)] = E[log P(X|z)] - D_{KL}[Q(z|X)‚à•P(z)]\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries to implement VAE\n",
    "\n",
    "- We are using Vanilla VAE, as it is the simplest form of VAE, as it does not use Neural network for implementing the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializing data and the dimensions for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "mb_size = 64\n",
    "z_dim = 16\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 256\n",
    "c = 0\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to plot images and to initilize data and xavier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='magma')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return prob, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Loss: 747.8\n",
      "\n",
      "Iter: 1000\n",
      "Loss: 118.5\n",
      "\n",
      "Iter: 2000\n",
      "Loss: 115.6\n",
      "\n",
      "Iter: 3000\n",
      "Loss: 112.8\n",
      "\n",
      "Iter: 4000\n",
      "Loss: 111.2\n",
      "\n",
      "Iter: 5000\n",
      "Loss: 112.2\n",
      "\n",
      "Iter: 6000\n",
      "Loss: 103.6\n",
      "\n",
      "Iter: 7000\n",
      "Loss: 109.6\n",
      "\n",
      "Iter: 8000\n",
      "Loss: 114.3\n",
      "\n",
      "Iter: 9000\n",
      "Loss: 114.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z_mu, z_logvar = Q(X)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample)\n",
    "X_samples, _ = P(z)\n",
    "\n",
    "recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "solver = tf.train.AdamOptimizer().minimize(vae_loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(10000):\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print()\n",
    "\n",
    "        samples = sess.run(X_samples, feed_dict={z: np.random.randn(16, z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Code Units for Latent space and 256 Hidden nodes\n",
    "\n",
    "<img src=\"out/2code.png\"></img>\n",
    "\n",
    "## 8 Code Units for Latent space and 256 Hidden nodes\n",
    "\n",
    "<img src=\"out/8code.png\"></img>\n",
    "\n",
    "## 16 Code Units for Latent space and 256 Hidden nodes\n",
    "\n",
    "<img src=\"out/16code.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. \n",
    "\n",
    "### 3. (Optional) Implement VAE to generate MNIST images, where you use convolutional neural network from encoder and deconvolutional neural network for decoder. This one should be super-easy (because there are many tutorials in the Internet and you have already solved problem 2) for you to catch up with the total score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import keras.initializers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test,10)\n",
    "vae.fit(x_train,x_train, shuffle=True,epochs=epochs,batch_size=batch_size,\n",
    "        validation_data=(x_test,x_test))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits generated by VAE\n",
    "\n",
    "<img src=\"VAE.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (Optional) Derive and explain the GAN algorithm. Implement GAN and train it from MNIST training data set to generate digits. Show images generated from GAN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='magma')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "\n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(50000):\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image generated by GAN\n",
    "\n",
    "<img src=\"GAN.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "1. https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/\n",
    "\n",
    "2. http://deeplearning.net/tutorial/rbm.html\n",
    "\n",
    "3. https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "4. Textbook , slides\n",
    "\n",
    "5. https://skymind.ai/wiki/restricted-boltzmann-machine\n",
    "\n",
    "6. https://rubikscode.net/2018/10/01/introduction-to-restricted-boltzmann-machines/\n",
    "\n",
    "7. https://rubikscode.net/2018/10/22/implementing-restricted-boltzmann-machine-with-python-and-tensorflow/\n",
    "\n",
    "8. WISEODD\n",
    "\n",
    "9. https://medium.com/@manivannan_data/resize-image-using-opencv-python-d2cdbbc480f0\n",
    "\n",
    "10. https://github.com/wiseodd/generative-models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
