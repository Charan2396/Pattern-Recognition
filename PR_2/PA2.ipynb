{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATTERN RECOGNITION\n",
    "\n",
    "## ASSIGNMENT - PROBLEM SET 2 \n",
    "\n",
    "### Linear Discriminant Functions and Support Vector Machines.\n",
    "\n",
    "# TASK 1 \n",
    "\n",
    "__The aim of this task to perform cross validation on the MNIST dataset using SVM.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T18:36:43.195404Z",
     "start_time": "2019-03-16T18:36:39.164626Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T18:36:43.518825Z",
     "start_time": "2019-03-16T18:36:43.197181Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train= loadlocal_mnist(images_path='train-images-idx3-ubyte',\n",
    "                                  labels_path='train-labels-idx1-ubyte')\n",
    "X_test, y_test=loadlocal_mnist(images_path='t10k-images-idx3-ubyte',\n",
    "                                labels_path='t10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T18:36:43.527676Z",
     "start_time": "2019-03-16T18:36:43.523410Z"
    }
   },
   "outputs": [],
   "source": [
    "clf=LinearSVC(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T18:39:06.875456Z",
     "start_time": "2019-03-16T18:36:43.532473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T18:39:06.950957Z",
     "start_time": "2019-03-16T18:39:06.877436Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T18:39:07.468179Z",
     "start_time": "2019-03-16T18:39:06.953335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  85.42\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ',clf.score(X_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __TASK 2__\n",
    "\n",
    "* The main aim of this task is to \n",
    "\\begin{align}\n",
    "\\text{minimize  } \\frac{1}{2}w^T.w+C \\sum_{n=1}^{N} \\xi_{i}\n",
    "\\\\\n",
    "\\text{subject to } y_i.(w^T.x_i) \\geq {1-\\xi_{i}} \\text{ and } \\xi_i \\geq 0 \\text{ for i }=1,...,N\n",
    "\\end{align}\n",
    "_The margin is_ :\n",
    " $\\gamma = \\frac{1}{\\sqrt{w^T.w +C \\sum_{i=1}^{N} \\xi_i }}$\n",
    " \n",
    " \n",
    "__Lagrange function :__\n",
    "\\begin{align}\n",
    "{\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
    "\\end{align}\n",
    "\n",
    "_Lagrange multipliers_ $\\alpha \\geq 0$, $ \\beta \\geq 0$,<br>\n",
    "_Inequality constraints_ $1 - y_i(w^Tx_i) - \\xi_i \\leq 0$ and $\\xi_i \\geq 0$ for $i=1,...,N$\n",
    "<br><br>\n",
    "__Claim:__ <br>\n",
    "<div align='center'>\n",
    "\n",
    "\n",
    "$\\underbrace{\\underset{\\alpha,\\beta}{max} \\underset{w,\\xi}{min} \\mathcal{L}}_\\text{dual solution} \\leq \\underbrace{\\underset{w,\\xi}{min} \\underset{\\alpha,\\beta}{max} \\mathcal{L}}_\\text{primal solution}$\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\\begin{align}\n",
    "{\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\\\\n",
    "  \\frac{\\partial_{\\mathcal{L}}}{\\partial_w} = w - \\sum_i \\alpha_i y_i x_i = 0 \\implies w = \\sum_i \\alpha_i y_i \\overrightarrow{x_i}\n",
    "  \\\\\n",
    "  \\frac{\\partial_{\\mathcal{L}}}{\\partial_\\xi} = 0 \\implies C - \\alpha_i - \\beta_i = 0\n",
    "  \\\\ \n",
    "  \\implies 0 \\leq \\alpha_i \\leq C \\text{ and } \\beta_i \\geq 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\text {Substituting } w = \\sum_i \\alpha_i y_i \\overrightarrow{x_i}, C=\\alpha_i + \\beta_i\n",
    "\\text { into Lagrange function, we get the dual problem of maximizing -  }\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "{\\mathcal{L}} = \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + (\\alpha_i + \\beta_i) \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
    "\\\\= \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + \\alpha_i\\sum_{i=1}^N\\xi_i + \\beta_i \\sum_{i=1}^N\\xi_i +\\sum_i \\alpha_i - w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} - \\sum_i \\alpha_i \\xi_i - \\sum_{i=1}^N \\beta_i \\xi_i\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\\\=\\sum_i \\alpha_i- \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j (\\overrightarrow{x_i}^T x_j )\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "__The primal margin is :__\n",
    " $\\gamma = \\frac{1}{\\sqrt{w^T.w +C \\sum_{i=1}^{N} \\xi_i }}$\n",
    " \n",
    "__The dual margin is :__\n",
    "$\\gamma = \\frac{1}{\\sqrt{\\alpha_i \\alpha_j y_i y_j (x_i^T x_j )}}$\n",
    "\n",
    "\n",
    "\n",
    "__1. Benifits of maximizing margin __\n",
    "\n",
    "A large margin effectively corresponds to a regularization of SVM weights which prevents overfitting. Hence, we prefer to maximize the margin because it helps us generalize our predictions and perform better on the test data by not overfitting the model to the training data.\n",
    "\n",
    "\n",
    "__2. Benifit for using dual problem instead of primal problem__\n",
    "\n",
    "It is easier to optimize in dual than in primal when the number of data points is lesser than the number of dimensions and also the primal cannot be used when a kernel is required. Dual problem also gives the weight of all the supporting vectors which does not happen in the primal problem. \n",
    "\n",
    "__3. Charaterizing the support vectors__\n",
    "\n",
    "There are mainly three types of support vectors for the above problem, they are defined by \n",
    "\n",
    "* The vectors that lie within the margin regions $ 0 < \\xi_n < 1 $ - the correct side of the margin.\n",
    "<br>\n",
    "* The vectors that lie on the boundaries $ w^T x + b = -1 $ and $ w^T x + b = +1 (\\xi_n)$\n",
    "* The vectors on the wrong side of the plane ($ \\xi_n \\geq 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3\n",
    "\n",
    "__The main of this task is to find the formal problem and derive the dual problem when there are multiple classes.__\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\underset{w_{m}\\in H,\\xi \\in R^l}{min} \\frac{1}{2} \\sum_{m=1}^{k} w_{m}^{T} w_{m} + C \\sum_{i=1}^{l}  \\xi_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "{\\text{subject to,   }}  w_{y_i}^T \\varphi(x_i) - w_{t}^{T}\\varphi(x_i) \\geq 1 - \\delta_{y_i,t} - \\xi_i\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "i = 1,...,l, t \\in {1,...,k},\n",
    "\\end{align}\n",
    "\n",
    "#### The resulting Decision function is:\n",
    "\\begin{align}\n",
    "argmax_m f_m(x) = argmax_m w^{T}_{m} \\varphi(x)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Note that the constraints $\\xi_i$ â‰¥ 0, i = 1,...,l, are implicitly indicated in the margin constraints of (12) when t equals $y_i$. In addition, (12) focuses on classification rule (15) without the bias terms bm, m = 1,...,k. A nonzero bm can be easily modeled by adding an additional constant feature to each x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "\n",
    "1. Class slides\n",
    "2. https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec8_slides.pdf\n",
    "3. https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
